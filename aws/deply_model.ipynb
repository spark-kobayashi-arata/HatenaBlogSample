{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=\"s3://blog-craft/blog_craft.tar.gz\",\n",
    "    role=role,\n",
    "    entry_point=\"inference.py\",\n",
    "    framework_version=\"1.12.1\",\n",
    "    py_version=\"py38\",\n",
    ")\n",
    "\n",
    "serverless_config = ServerlessInferenceConfig(\n",
    "    # メモリ容量\n",
    "    # 容量が大きいほど料金が高い\n",
    "    # デフォルトでは1024/2048/3072の3つから選択\n",
    "    memory_size_in_mb=3072,\n",
    "    # 同時実行可能なインスタンス数\n",
    "    # デフォルトでは最大10インスタンス\n",
    "    max_concurrency=1,\n",
    ")\n",
    "\n",
    "deploy_params = {\n",
    "    \"instance_type\": \"ml.t3.medium\",\n",
    "    \"initial_instance_count\": 1,\n",
    "    \"serverless_inference_config\": serverless_config,\n",
    "}\n",
    "\n",
    "predictor = pytorch_model.deploy(**deploy_params)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
